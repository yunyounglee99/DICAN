import sys
import os
import numpy as np
import torch
from torchvision import transforms
from torch.utils.data import Dataset
import torch.nn.functional as F

# Mammoth í•„ìˆ˜ ìœ í‹¸ë¦¬í‹°
from datasets.utils.continual_dataset import ContinualDataset, store_masked_loaders

# -----------------------------------------------------------------------------
# [ê²½ë¡œ ì„¤ì •] í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë° ë°ì´í„°ì…‹ ê²½ë¡œ
# -----------------------------------------------------------------------------
PROJECT_ROOT = '/root/DICAN'
DATA_ROOT_DDR = '/root/DICAN_DATASETS/DDR'     # Base Session (DDR)
DATA_ROOT_INC = '/root/DICAN_DATASETS'         # Inc Session (APTOS, etc.)

if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

# DICAN ì»¤ìŠ¤í…€ ë¡œë” ì„í¬íŠ¸
try:
    from data.base_loader import DDRBaseDataset
    from data.inc_loader import UnifiedIncrementalDataset
except ImportError as e:
    print(f"[Error] DICAN ë°ì´í„° ë¡œë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {PROJECT_ROOT}")
    raise e

# ê¸°ì¡´ import ì•„ë˜ì— ì¶”ê°€
import torch

class MammothWrapper(Dataset):
    """
    [Standard Model Compatible Wrapper]
    EWC, ER ë“± ì¼ë°˜ Mammoth ëª¨ë¸ì„ ìœ„í•œ ë˜í¼ì…ë‹ˆë‹¤.
    1. ë°˜í™˜ê°’: (image, label, original_image) -> ì´ 3ê°œ í•„ìˆ˜!
    2. ì…ë ¥í˜•íƒœ: ìˆœìˆ˜ 3ì±„ë„ RGB ì´ë¯¸ì§€ (ë§ˆìŠ¤í¬ ì œê±°)
    """
    def __init__(self, dataset):
        self.dataset = dataset
        
        # Mammoth í•„ìˆ˜ ì†ì„± (Data Splittingìš©)
        if hasattr(dataset, 'data_map'): # Base (DDR)
            src = dataset.data_map
            self.data = np.array([d['img_name'] for d in src])
            self.targets = np.array([d['label'] for d in src])
        elif hasattr(dataset, 'data_list'): # Inc (APTOS...)
            src = dataset.data_list
            self.data = np.array([d['path'] for d in src])
            self.targets = np.array([d['label'] for d in src])
        else:
            raise ValueError(f"Unknown dataset type: {type(dataset)}")

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        data = self.dataset[idx]
        
        # 1. ì´ë¯¸ì§€ (Tensor: [3, 224, 224])
        image = data['image']
        
        # 2. ë¼ë²¨ (Int/Long)
        label = data['label']
        
        # [í•µì‹¬ ìˆ˜ì • 1] ë§ˆìŠ¤í¬ ì œê±° (Standard ëª¨ë¸ì€ 7ì±„ë„ ì…ë ¥ì„ ëª» ë°›ìŒ)
        # EWCëŠ” Classification ëª¨ë¸ì´ë¯€ë¡œ ë§ˆìŠ¤í¬ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
        
        # [í•µì‹¬ ìˆ˜ì • 2] 3ë²ˆì§¸ ì¸ì ë°˜í™˜ (Mammoth ìš”êµ¬ì‚¬í•­)
        # MammothëŠ” ë‚´ë¶€ì ìœ¼ë¡œ (input, label, not_aug_input)ì„ ê¸°ëŒ€í•©ë‹ˆë‹¤.
        # ë³„ë„ì˜ Augmentationì´ ì—†ë‹¤ë©´ imageë¥¼ ê·¸ëŒ€ë¡œ í•œ ë²ˆ ë” ë°˜í™˜í•˜ë©´ í•´ê²°ë©ë‹ˆë‹¤.
        
        return image, label, image

class DRGrading(ContinualDataset):
    """
    Continual Learningì„ ìœ„í•œ ë©”ì¸ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (Mammoth í˜¸í™˜)
    Task 0: DDR (Base Session, with Masks)
    Task 1+: APTOS, Messidor-2, DRAC22 (Incremental Session, No Masks)
    """
    NAME = 'dr-grading'
    SETTING = 'domain-il'  # Domain-Incremental Learning
    N_CLASSES_PER_TASK = 5 
    N_TASKS = 4 
    SIZE = (3, 224, 224)

    def get_data_loaders(self):
        task_id = self.current_task
        print(f"\n[Mammoth] Loading Data for Task {task_id}...")

        if task_id == 0:
            # ==========================================
            # Task 0: Base Session (DDR) - Local Load
            # ==========================================
            print(f"[*] Loading DDR Dataset from: {DATA_ROOT_DDR}")
            
            # base_loader.pyì˜ DDRBaseDataset ì‚¬ìš©
            # [ì£¼ì˜] Test Transformì€ Mammoth ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë˜ê±°ë‚˜ ë¡œë” ì•ˆì—ì„œ ì²˜ë¦¬ë¨.
            # ì—¬ê¸°ì„œëŠ” ë¡œë”ê°€ ì´ë¯¸ Transformì„ í¬í•¨í•˜ê³  ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©.
            train_raw = DDRBaseDataset(root_dir=DATA_ROOT_DDR, split='train', img_size=224)
            test_raw = DDRBaseDataset(root_dir=DATA_ROOT_DDR, split='valid', img_size=224)

        else:
            # ==========================================
            # Task 1+: Incremental Session
            # ==========================================
            # Task ID ë§¤í•‘ (1 -> APTOS, 2 -> Messidor, 3 -> DRAC)
            # UnifiedIncrementalDatasetì€ session_id 1, 2, 3ì„ ë°›ìŒ
            session_map = {1: "APTOS 2019", 2: "Messidor-2", 3: "DRAC22"}
            session_name = session_map.get(task_id, f"Session {task_id}")
            
            print(f"[*] Loading Incremental Dataset: {session_name}")
            print(f"    -> Path: {DATA_ROOT_INC}")

            # inc_loader.pyì˜ UnifiedIncrementalDataset ì‚¬ìš©
            train_raw = UnifiedIncrementalDataset(
                session_id=task_id,
                data_dir=DATA_ROOT_INC,
                img_size=224,
                shot=10,        # Few-shot ì„¤ì • (í•„ìš”ì‹œ argsì—ì„œ ë°›ì•„ì˜¤ê²Œ ìˆ˜ì • ê°€ëŠ¥)
                split='train'
            )
            # Test ì…‹ì€ Shot ì œí•œ ì—†ì´ ì „ì²´ ì‚¬ìš©
            test_raw = UnifiedIncrementalDataset(
                session_id=task_id,
                data_dir=DATA_ROOT_INC,
                img_size=224,
                shot=None, 
                split='test'    # ë˜ëŠ” validation êµ¬ì¡°ì— ë”°ë¼ 'val' ì‚¬ìš©
            )

        # 2. Mammoth í˜¸í™˜ ë˜í¼ ì ìš©
        train_dataset = MammothWrapper(train_raw)
        test_dataset = MammothWrapper(test_raw)

        # 3. DataLoader ìƒì„± (Mammoth ìœ í‹¸ë¦¬í‹° ì‚¬ìš©)
        # store_masked_loadersëŠ” ë‚´ë¶€ì ìœ¼ë¡œ Class Incremental ë“±ì˜ ë§ˆìŠ¤í‚¹ì„ ì²˜ë¦¬í•˜ì§€ë§Œ,
        # Domain-IL ì„¤ì •ì—ì„œëŠ” ì „ì²´ í´ë˜ìŠ¤ë¥¼ ë‹¤ ë³´ì—¬ì£¼ë„ë¡ ë™ì‘í•¨.
        train_loader, test_loader = store_masked_loaders(train_dataset, test_dataset, self)

        # [ì•ˆì „ì¥ì¹˜] ë¡œë”ì˜ ì›Œì»¤ ìˆ˜ ì¡°ì • (ë¡œì»¬ íŒŒì¼ì´ë¯€ë¡œ 4~8 ì ë‹¹)
        # Mammothê°€ ë‚´ë¶€ì ìœ¼ë¡œ ìƒì„±í•œ ë¡œë” ì†ì„±ì„ ë®ì–´ì”Œì›€
        if hasattr(train_loader, 'num_workers'): train_loader.num_workers = 4
        if hasattr(test_loader, 'num_workers'): test_loader.num_workers = 4

        # [í™•ì¸ìš© ë¡œê·¸]
        print(f"\n[*] ğŸ” Checking Labels for Task {task_id}...")
        train_labels = np.unique(train_dataset.targets)
        print(f"    -> [Train] Unique Labels: {train_labels} (Should be 0-4)")

        return train_loader, test_loader

    def get_transform(self):
        # ë°ì´í„°ì…‹ ë‚´ë¶€ì—ì„œ ì´ë¯¸ transformì„ ìˆ˜í–‰í•˜ë¯€ë¡œ, 
        # Mammothê°€ ì¶”ê°€ì ìœ¼ë¡œ transformì„ ì ìš©í•˜ì§€ ì•Šë„ë¡ Identity ë°˜í™˜
        return transforms.Compose([])

    @staticmethod
    def get_loss():
        return F.cross_entropy

    @staticmethod
    def get_normalization_transform():
        # ì—­ì‹œ ë°ì´í„°ì…‹ ë‚´ë¶€ì—ì„œ Normalizeê¹Œì§€ ëë‚œ ìƒíƒœì„.
        return transforms.Compose([])
    
    @staticmethod
    def get_denormalization_transform():
        # ì‹œê°í™” ë“±ì„ ìœ„í•´ ì—­ë³€í™˜ í•„ìš”ì‹œ ì‚¬ìš©
        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]
        return transforms.Compose([
            transforms.Normalize(mean=[-m/s for m, s in zip(mean, std)],
                                 std=[1/s for s in std])
        ])