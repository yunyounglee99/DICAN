"""
Using Example :

python train.py \
  --data_path /Volumes/Nyoungs_SSD/macbook/dev/datasets/DICAN_DATASETS/DDR \
  --dataset DDR \
  --epochs_base 20 \
  --batch_size 32 \
  --n_tasks 4 \
  --n_shot 10 \
  --device cuda
  
"""

import argparse
import os
import torch
import numpy as np
import random
from torch.utils.data import DataLoader

# -----------------------------------------------------------------------------
# 1. ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ Import (íŒŒì¼ ê²½ë¡œ ê¸°ë°˜)
# -----------------------------------------------------------------------------
from data.base_loader import DDRBaseDataset
from data.inc_loader import get_incremental_loader
from models.dican_cbm import DICAN_CBM
from train_base import BaseTrainer
from train_incremental import IncrementalTrainer

# -----------------------------------------------------------------------------
# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# -----------------------------------------------------------------------------
def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def get_args():
    parser = argparse.ArgumentParser(description="DICAN Training Pipeline")
    
    # [Data]
    parser.add_argument('--dataset', type=str, default='DDR', help='Base dataset name')
    parser.add_argument('--data_path', type=str, required=True, help='Root path of DICAN_DATASETS')
    parser.add_argument('--save_path', type=str, default='./checkpoints', help='Save path')
    
    # [System]
    parser.add_argument('--device', type=str, default='cuda', help='cuda or cpu')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    parser.add_argument('--num_workers', type=int, default=4, help='Dataloader workers')
    
    # [Model]
    parser.add_argument('--backbone', type=str, default='resnet50', help='Backbone type')
    parser.add_argument('--n_concepts', type=int, default=4, help='Number of concepts (bottleneck)')
    parser.add_argument('--num_classes', type=int, default=5, help='Number of DR grades')
    
    # [Base Training]
    parser.add_argument('--epochs_base', type=int, default=20, help='Base training epochs')
    parser.add_argument('--batch_size', type=int, default=32, help='Base batch size')
    parser.add_argument('--lr_base', type=float, default=1e-4, help='Base learning rate')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')
    parser.add_argument('--lambda_c', type=float, default=1.0, help='Concept loss weight')
    
    # [Incremental Training]
    parser.add_argument('--n_tasks', type=int, default=4, help='Total tasks (Base + 3 Inc)')
    parser.add_argument('--n_shot', type=int, default=10, help='Few-shot count')
    parser.add_argument('--lr_inc', type=float, default=1e-3, help='Incremental learning rate')
    parser.add_argument('--adaptation_steps', type=int, default=100, help='Adaptation steps')
    
    return parser.parse_args()

# -----------------------------------------------------------------------------
# 3. ë°ì´í„° ë¡œë” ë˜í¼ (Loader Wrapper)
# -----------------------------------------------------------------------------
class IncLoaderManager:
    """
    IncrementalTrainerê°€ ìš”êµ¬í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤(get_incremental_loaders)ë¥¼
    data/inc_loader.pyì˜ í•¨ìˆ˜ì™€ ì—°ê²°í•´ì£¼ëŠ” ë˜í¼ í´ë˜ìŠ¤
    """
    def __init__(self, args):
        self.args = args
        # Task IDì™€ Session ID ë§¤í•‘ (Task 1 -> Session 1: APTOS, etc.)
        self.task_to_session = {
            1: 1, # APTOS
            2: 2, # Messidor-2
            3: 3  # DRAC22
        }

    def get_incremental_loaders(self, task_id):
        session_id = self.task_to_session.get(task_id)
        if session_id is None:
            raise ValueError(f"[Error] No session mapped for Task ID {task_id}")

        print(f"[*] Loading Incremental Data for Task {task_id} (Session {session_id})...")
        
        # Support Set (Train, Few-shot)
        support_loader = get_incremental_loader(
            session_id=session_id,
            data_root=self.args.data_path,
            mode='train',
            batch_size=self.args.batch_size, # ë©”ëª¨ë¦¬ê°€ ì ë‹¤ë©´ ì¤„ì—¬ì•¼ í•¨
            shot=self.args.n_shot
        )
        
        # Query Set (Test, All Data)
        query_loader = get_incremental_loader(
            session_id=session_id,
            data_root=self.args.data_path,
            mode='test', # ê³µì •í•œ í‰ê°€ë¥¼ ìœ„í•´ test ì…‹ ì‚¬ìš©
            batch_size=self.args.batch_size,
            shot=None
        )
        
        return support_loader, query_loader

def get_base_loaders(args):
    """base_loader.pyì˜ DDRBaseDatasetì„ DataLoaderë¡œ í¬ì¥"""
    print(f"[*] Loading Base Data (DDR) from {args.data_path}...")
    
    train_ds = DDRBaseDataset(root_dir=args.data_path, split='train')
    val_ds = DDRBaseDataset(root_dir=args.data_path, split='valid')
    
    train_loader = DataLoader(
        train_ds, 
        batch_size=args.batch_size, 
        shuffle=True, 
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_ds, 
        batch_size=args.batch_size, 
        shuffle=False, 
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    return train_loader, val_loader

# -----------------------------------------------------------------------------
# 4. Main Function
# -----------------------------------------------------------------------------
def main():
    args = get_args()
    set_seed(args.seed)
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    
    if not os.path.exists(args.save_path):
        os.makedirs(args.save_path)

    print("\n" + "="*50)
    print(f"ğŸš€ DICAN Training Start")
    print(f"   - Data Root: {args.data_path}")
    print(f"   - Device: {device}")
    print(f"   - Concepts: {args.n_concepts}")
    print("="*50 + "\n")

    # 1. ëª¨ë¸ ì´ˆê¸°í™”
    model = DICAN_CBM(
        num_concepts=args.n_concepts,
        num_classes=args.num_classes,
        feature_dim=2048 # ResNet50 ê¸°ì¤€
    ).to(device)

    # -------------------------------------------------------
    # [Phase 1] Base Training (Task 0)
    # -------------------------------------------------------
    # 1-1. Base Loader ì¤€ë¹„
    train_loader, val_loader = get_base_loaders(args)
    
    # 1-2. Mode ì„¤ì • (Backbone/Head í•™ìŠµ, Projector Freeze)
    model.set_session_mode('base')
    
    # 1-3. Trainer ì‹¤í–‰
    base_trainer = BaseTrainer(args, model, device, train_loader, val_loader)
    
    # [ì¶”ê°€ ìš”ì²­] ë°ì´í„° í†µê³„ í™•ì¸
    if hasattr(base_trainer, 'check_data_statistics'):
        base_trainer.check_data_statistics()
        
    model = base_trainer.run() # í•™ìŠµ ë° Prototype ì´ˆê¸°í™” ì™„ë£Œ

    # -------------------------------------------------------
    # [Phase 2] Incremental Learning (Task 1 ~ N)
    # -------------------------------------------------------
    print("\n" + "="*50)
    print(f"ğŸ”„ Starting Incremental Phase (Total {args.n_tasks-1} tasks)")
    print("="*50)
    
    # 2-1. Inc Loader ì¤€ë¹„
    inc_loader_manager = IncLoaderManager(args)
    
    # 2-2. Mode ì„¤ì • (Projector í•™ìŠµ, Backbone/Head Freeze)
    model.set_session_mode('incremental')
    
    # 2-3. Trainer ì´ˆê¸°í™” (PrototypeBank ì—°ë™)
    inc_trainer = IncrementalTrainer(args, model, device, inc_loader_manager)
    
    acc_history = []
    
    # Task Loop
    for task_id in range(1, args.n_tasks):
        # train_task ë‚´ë¶€ì—ì„œ check_data_statistics í˜¸ì¶œë¨ (ìˆ˜ì •ëœ ì½”ë“œ ê¸°ì¤€)
        acc = inc_trainer.train_task(task_id)
        acc_history.append(acc)
        
        print(f"ğŸ“ˆ [Result] Task {task_id} Accuracy: {acc:.2f}%")
        print(f"    Current Avg Acc: {sum(acc_history)/len(acc_history):.2f}%")

    print("\n" + "="*50)
    print("ğŸ‰ All Training Finished!")
    print(f"   - Final Average Accuracy: {sum(acc_history)/len(acc_history):.2f}%")
    print("="*50)

if __name__ == "__main__":
    main()