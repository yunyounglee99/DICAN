"""
DICAN DR Grading Dataset for Mammoth Framework
================================================
[ë³€ê²½ì‚¬í•­]
- Task 0 (Base Session) ë°ì´í„° ë¶„ë¦¬:
  â˜… Phase 1-A: FGADRë§Œ (100% ë§ˆìŠ¤í¬) â†’ backbone + seg í•™ìŠµ
  â˜… Phase 1-B/C: DDR+FGADR ì „ì²´ â†’ prototype ì¶”ì¶œ + classification
- QWK (Quadratic Weighted Kappa) ë©”íŠ¸ë¦­ ì¶”ê°€
- FGADR loader ì—°ë™ (fgadr_loader.py)

[Mammoth ë¹„êµì‹¤í—˜ìš©]
EWC, LwF, L2P, DualPrompt ë“± ë‹¤ë¥¸ CL ëª¨ë¸ì€ seg ì—†ì´ classificationë§Œ í•˜ë¯€ë¡œ
Task 0ì—ì„œ DDR+FGADR ì „ì²´(full)ë¥¼ ì‚¬ìš©.
DICANë§Œ Phase 1-Aì—ì„œ seg_loaderë¥¼ ë³„ë„ë¡œ ì“°ëŠ” êµ¬ì¡°.
"""

import sys
import os
import numpy as np
import torch
from torchvision import transforms
from torch.utils.data import Dataset, ConcatDataset
import torch.nn.functional as F
from sklearn.metrics import cohen_kappa_score

# Mammoth í•„ìˆ˜ ìœ í‹¸ë¦¬í‹°
from datasets.utils.continual_dataset import ContinualDataset, store_masked_loaders

# -----------------------------------------------------------------------------
# [ê²½ë¡œ ì„¤ì •]
# -----------------------------------------------------------------------------
PROJECT_ROOT = '/root/DICAN'
DATA_ROOT_DDR = '/root/DICAN_DATASETS/DDR'
DATA_ROOT_FGADR = '/root/DICAN_DATASETS/FGADR'
DATA_ROOT_INC = '/root/DICAN_DATASETS'

if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

try:
    from data.base_loader import DDRBaseDataset
    from data.fgadr_loader import FGADRSegDataset
    from data.inc_loader import UnifiedIncrementalDataset
except ImportError as e:
    print(f"[Error] DICAN ë°ì´í„° ë¡œë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PROJECT_ROOT}")
    raise e


# =============================================================================
# Mammoth í˜¸í™˜ ë˜í¼
# =============================================================================
class MammothWrapper(Dataset):
    """
    Standard CL ëª¨ë¸(EWC, LwF ë“±) í˜¸í™˜ ë˜í¼.
    ë°˜í™˜: (image, label, original_image) â€” Mammoth í•„ìˆ˜ 3-tuple.
    ë§ˆìŠ¤í¬ëŠ” ì œê±° (standard ëª¨ë¸ì€ 3ì±„ë„ RGBë§Œ ì…ë ¥).
    """
    def __init__(self, dataset):
        self.dataset = dataset
        
        if hasattr(dataset, 'data_map'):       # DDR
            src = dataset.data_map
            self.data = np.array([d['img_name'] for d in src])
            self.targets = np.array([d['label'] for d in src])
        elif hasattr(dataset, 'data_list'):    # Inc (APTOS, Messidor, DRAC)
            src = dataset.data_list
            self.data = np.array([d['path'] for d in src])
            self.targets = np.array([d['label'] for d in src])
        else:
            raise ValueError(f"Unknown dataset type: {type(dataset)}")

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        data = self.dataset[idx]
        image = data['image']
        label = data['label']
        return image, label, image


class MammothConcatWrapper(Dataset):
    """
    ConcatDataset(DDR + FGADR)ì„ Mammoth í˜¸í™˜ìœ¼ë¡œ ê°ì‹¸ëŠ” ë˜í¼.
    ConcatDatasetì€ data_map/data_list ì†ì„±ì´ ì—†ìœ¼ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬.
    """
    def __init__(self, concat_dataset):
        self.dataset = concat_dataset
        
        # Mammoth í•„ìˆ˜ ì†ì„± êµ¬ì¶•: targets ë°°ì—´
        targets = []
        for i in range(len(concat_dataset)):
            # ê° ì„œë¸Œ ë°ì´í„°ì…‹ì˜ data_map/data_listì—ì„œ ë¼ë²¨ ì¶”ì¶œ
            idx = i
            for ds in concat_dataset.datasets:
                if idx < len(ds):
                    if hasattr(ds, 'data_map'):
                        targets.append(ds.data_map[idx]['label'])
                    elif hasattr(ds, 'data_list'):
                        targets.append(ds.data_list[idx]['label'])
                    else:
                        # fallback: ì‹¤ì œë¡œ ë¡œë“œ
                        sample = ds[idx]
                        targets.append(sample['label'] if isinstance(sample['label'], int) 
                                     else sample['label'].item())
                    break
                idx -= len(ds)
        
        self.targets = np.array(targets)
        self.data = np.arange(len(concat_dataset))  # ì¸ë±ìŠ¤ë¥¼ dataë¡œ ì‚¬ìš©

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        data = self.dataset[idx]
        image = data['image']
        label = data['label']
        return image, label, image


# =============================================================================
# QWK ë©”íŠ¸ë¦­ ìœ í‹¸ë¦¬í‹°
# =============================================================================
def compute_qwk(model, loader, device):
    """
    ëª¨ë¸ í‰ê°€ ì‹œ QWK ê³„ì‚°.
    Mammoth evaluate() ì´í›„ ì¶”ê°€ë¡œ í˜¸ì¶œ ê°€ëŠ¥.
    
    Returns:
        dict: {'accuracy': float, 'qwk': float}
    """
    model.eval()
    all_preds = []
    all_labels = []
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch in loader:
            if isinstance(batch, (list, tuple)):
                images, labels = batch[0], batch[1]
            elif isinstance(batch, dict):
                images, labels = batch['image'], batch['label']
            else:
                continue
            
            images = images.to(device)
            labels = labels.to(device) if isinstance(labels, torch.Tensor) else torch.tensor(labels).to(device)
            
            outputs = model(images)
            if isinstance(outputs, dict):
                logits = outputs.get('logits', outputs.get('output', None))
                if logits is None:
                    logits = list(outputs.values())[0]
            elif isinstance(outputs, torch.Tensor):
                logits = outputs
            else:
                logits = outputs[0] if isinstance(outputs, (list, tuple)) else outputs
            
            _, predicted = logits.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    acc = 100.0 * correct / total if total > 0 else 0.0
    qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic') if total > 0 else 0.0
    
    return {'accuracy': acc, 'qwk': qwk}


# =============================================================================
# ë©”ì¸ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# =============================================================================
class DRGrading(ContinualDataset):
    """
    Mammoth í˜¸í™˜ DR Grading ë°ì´í„°ì…‹.
    
    [Task êµ¬ì„±]
    Task 0: Base Session
      - ë¹„êµ ëª¨ë¸(EWC, LwF ë“±): DDR+FGADR ì „ì²´ (classification only)
      - DICAN: Phase 1-A(FGADR only, seg) â†’ 1-B/C(DDR+FGADR, cls)
              â†’ DICANì€ ìì²´ train.pyë¡œ í•™ìŠµ, ì—¬ê¸°ì„œëŠ” ë¹„êµ ëª¨ë¸ìš©
    Task 1: APTOS 2019
    Task 2: Messidor-2
    Task 3: DRAC22
    
    [QWK ì§€ì›]
    get_data_loaders() ë°˜í™˜ ì‹œ self.test_loadersì— ì €ì¥í•˜ì—¬
    evaluate_with_qwk()ë¡œ ì „ì²´ task QWK ê³„ì‚° ê°€ëŠ¥.
    """
    NAME = 'dr-grading'
    SETTING = 'domain-il'
    N_CLASSES_PER_TASK = 5
    N_TASKS = 4
    SIZE = (3, 224, 224)
    
    # â˜… QWK ì¶”ì ìš©
    _test_loaders = {}

    def get_data_loaders(self):
        task_id = self.current_task
        print(f"\n[Mammoth] Loading Data for Task {task_id}...")

        if task_id == 0:
            # ==================================================
            # Task 0: Base Session
            # â˜… DDR + FGADR ì „ì²´ ì‚¬ìš© (ë¹„êµ ëª¨ë¸ìš©)
            # DICANì€ ìì²´ íŒŒì´í”„ë¼ì¸ì—ì„œ FGADR-only segë¥¼ ì²˜ë¦¬
            # ==================================================
            print(f"[*] Loading Base Dataset: DDR + FGADR")
            print(f"    DDR:   {DATA_ROOT_DDR}")
            print(f"    FGADR: {DATA_ROOT_FGADR}")
            
            ddr_train = DDRBaseDataset(root_dir=DATA_ROOT_DDR, split='train', img_size=224)
            ddr_val = DDRBaseDataset(root_dir=DATA_ROOT_DDR, split='valid', img_size=224)
            
            use_fgadr = os.path.exists(DATA_ROOT_FGADR)
            
            if use_fgadr:
                fgadr_train = FGADRSegDataset(root_dir=DATA_ROOT_FGADR, split='train', img_size=224)
                fgadr_val = FGADRSegDataset(root_dir=DATA_ROOT_FGADR, split='valid', img_size=224)
                
                train_concat = ConcatDataset([ddr_train, fgadr_train])
                test_concat = ConcatDataset([ddr_val, fgadr_val])
                
                train_dataset = MammothConcatWrapper(train_concat)
                test_dataset = MammothConcatWrapper(test_concat)
                
                print(f"    âœ… Combined: Train={len(train_concat)}, Val={len(test_concat)}")
                print(f"       DDR  Train:{len(ddr_train)} Val:{len(ddr_val)}")
                print(f"       FGADR Train:{len(fgadr_train)} Val:{len(fgadr_val)}")
            else:
                print(f"    âš ï¸ FGADR not found, using DDR only")
                train_dataset = MammothWrapper(ddr_train)
                test_dataset = MammothWrapper(ddr_val)

        else:
            # ==================================================
            # Task 1+: Incremental Session
            # ==================================================
            session_map = {1: "APTOS 2019", 2: "Messidor-2", 3: "DRAC22"}
            session_name = session_map.get(task_id, f"Session {task_id}")
            
            print(f"[*] Loading Incremental Dataset: {session_name}")
            print(f"    -> Path: {DATA_ROOT_INC}")

            train_raw = UnifiedIncrementalDataset(
                session_id=task_id,
                data_dir=DATA_ROOT_INC,
                img_size=224,
                shot=10,
                split='train'
            )
            test_raw = UnifiedIncrementalDataset(
                session_id=task_id,
                data_dir=DATA_ROOT_INC,
                img_size=224,
                shot=None,
                split='test'
            )
            
            train_dataset = MammothWrapper(train_raw)
            test_dataset = MammothWrapper(test_raw)

        # DataLoader ìƒì„± (Mammoth ìœ í‹¸ë¦¬í‹°)
        train_loader, test_loader = store_masked_loaders(train_dataset, test_dataset, self)
        
        if hasattr(train_loader, 'num_workers'):
            train_loader.num_workers = 4
        if hasattr(test_loader, 'num_workers'):
            test_loader.num_workers = 4

        # â˜… QWK ê³„ì‚°ìš©ìœ¼ë¡œ test_loader ì €ì¥
        DRGrading._test_loaders[task_id] = test_loader

        # ë¼ë²¨ í™•ì¸ ë¡œê·¸
        print(f"\n[*] ğŸ” Task {task_id} Labels:")
        train_labels = np.unique(train_dataset.targets)
        print(f"    -> [Train] Unique Labels: {train_labels} (Should be 0-4)")
        print(f"    -> [Train] Size: {len(train_dataset)}")
        print(f"    -> [Test]  Size: {len(test_dataset)}")

        return train_loader, test_loader

    # =================================================================
    # â˜… QWK í‰ê°€ ë©”ì„œë“œ
    # =================================================================
    @classmethod
    def evaluate_with_qwk(cls, model, device, task_ids=None):
        """
        ì „ì²´ ë˜ëŠ” íŠ¹ì • taskì— ëŒ€í•´ Accuracy + QWK ê³„ì‚°.
        
        Usage:
            # í•™ìŠµ í›„ í˜¸ì¶œ
            results = DRGrading.evaluate_with_qwk(model, device)
            for tid, metrics in results.items():
                print(f"Task {tid}: Acc={metrics['accuracy']:.2f}%, QWK={metrics['qwk']:.4f}")
        
        Args:
            model: í•™ìŠµëœ ëª¨ë¸
            device: torch.device
            task_ids: í‰ê°€í•  task ID ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì €ì¥ëœ ëª¨ë“  task)
        
        Returns:
            dict: {task_id: {'accuracy': float, 'qwk': float}}
        """
        if task_ids is None:
            task_ids = sorted(cls._test_loaders.keys())
        
        results = {}
        print(f"\n{'='*55}")
        print(f"  QWK Evaluation (Tasks: {task_ids})")
        print(f"{'='*55}")
        
        for tid in task_ids:
            if tid not in cls._test_loaders:
                print(f"  Task {tid}: No test loader available (skipped)")
                continue
            
            loader = cls._test_loaders[tid]
            metrics = compute_qwk(model, loader, device)
            results[tid] = metrics
            
            print(f"  Task {tid}: Acc={metrics['accuracy']:.2f}%  |  QWK={metrics['qwk']:.4f}")
        
        # í‰ê· 
        if results:
            avg_acc = np.mean([m['accuracy'] for m in results.values()])
            avg_qwk = np.mean([m['qwk'] for m in results.values()])
            print(f"  {'â”€'*50}")
            print(f"  Average: Acc={avg_acc:.2f}%  |  QWK={avg_qwk:.4f}")
        
        print(f"{'='*55}\n")
        return results

    @classmethod
    def get_seg_loaders(cls, batch_size=32, num_workers=4):
        """
        â˜… DICAN Phase 1-A ì „ìš©: FGADR-only seg loader ë°˜í™˜.
        
        ë¹„êµ ëª¨ë¸(EWC ë“±)ì€ ì´ ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ.
        DICANì˜ ìì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì—ì„œë§Œ ì‚¬ìš©.
        
        Returns:
            dict: {'seg_train': DataLoader, 'seg_val': DataLoader}
                  ë˜ëŠ” FGADR ì—†ìœ¼ë©´ None
        """
        if not os.path.exists(DATA_ROOT_FGADR):
            print(f"[Warning] FGADR not found: {DATA_ROOT_FGADR}")
            return None
        
        from torch.utils.data import DataLoader
        
        fgadr_train = FGADRSegDataset(root_dir=DATA_ROOT_FGADR, split='train', img_size=224)
        fgadr_val = FGADRSegDataset(root_dir=DATA_ROOT_FGADR, split='valid', img_size=224)
        
        seg_train_loader = DataLoader(
            fgadr_train, batch_size=batch_size, shuffle=True,
            num_workers=num_workers, pin_memory=True
        )
        seg_val_loader = DataLoader(
            fgadr_val, batch_size=batch_size, shuffle=False,
            num_workers=num_workers, pin_memory=True
        )
        
        print(f"[*] FGADR Seg Loaders: Train={len(fgadr_train)}, Val={len(fgadr_val)}")
        
        return {
            'seg_train': seg_train_loader,
            'seg_val': seg_val_loader,
        }

    def get_transform(self):
        return transforms.Compose([])

    @staticmethod
    def get_loss():
        return F.cross_entropy

    @staticmethod
    def get_normalization_transform():
        return transforms.Compose([])

    @staticmethod
    def get_denormalization_transform():
        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]
        return transforms.Compose([
            transforms.Normalize(mean=[-m/s for m, s in zip(mean, std)],
                                 std=[1/s for s in std])
        ])